//! Main Voxtral Realtime model implementation.

use std::path::Path;

use candle_core::{DType, IndexOp, Tensor};
use candle_nn::{Module, VarBuilder};
use tracing::info;

use crate::audio::{MelConfig, MelSpectrogram};
use crate::error::{Error, Result};
use crate::models::device::DeviceProfile;
use crate::models::qwen3::{Qwen3Cache, Qwen3Model};

use super::audio::{AudioLanguageAdapter, TimeEmbedding};
use super::config::VoxtralConfig;
use super::tokenizer::{AudioConfig, VoxtralTokenizer};

/// Voxtral Realtime Model
pub struct VoxtralRealtimeModel {
    device: DeviceProfile,
    dtype: DType,
    tokenizer: VoxtralTokenizer,
    config: VoxtralConfig,
    whisper_encoder: WhisperEncoder,
    audio_adapter: AudioLanguageAdapter,
    language_model: Qwen3Model,
    time_embedding: TimeEmbedding,
    mel: MelSpectrogram,
}

impl VoxtralRealtimeModel {
    /// Load model from directory
    pub fn load(model_dir: &Path, device: DeviceProfile) -> Result<Self> {
        // Try params.json (Voxtral format) first, then config.json (standard)
        let config_path = if model_dir.join("params.json").exists() {
            model_dir.join("params.json")
        } else {
            model_dir.join("config.json")
        };
        let config_str = std::fs::read_to_string(&config_path)
            .map_err(|e| Error::ModelLoadError(format!("Failed to read config: {}", e)))?;
        let config: VoxtralConfig = serde_json::from_str(&config_str)
            .map_err(|e| Error::ModelLoadError(format!("Failed to parse config: {}", e)))?;

        // Setup audio processing
        let audio_cfg = config.audio_config();
        let mel_cfg = MelConfig {
            sample_rate: audio_cfg.sampling_rate,
            n_fft: audio_cfg.window_size,
            hop_length: audio_cfg.hop_length,
            n_mels: audio_cfg.num_mel_bins,
            f_min: 0.0,
            f_max: 8000.0,
            normalize: true,
        };
        let mel = MelSpectrogram::new(mel_cfg)?;

        // Setup tokenizer
        let audio_config = AudioConfig {
            sampling_rate: audio_cfg.sampling_rate,
            frame_rate: config.frame_rate(),
            window_size: audio_cfg.window_size,
            hop_length: audio_cfg.hop_length,
            num_mel_bins: audio_cfg.num_mel_bins,
            n_delay_tokens: config.num_delay_tokens(),
        };
        let tokenizer = VoxtralTokenizer::new(config.text_config().vocab_size, audio_config);

        let dtype = device.select_dtype(None);

        // Load weights - clone device to a local binding for lifetime
        let device_clone = device.clone();
        let vb = load_weights(model_dir, dtype, &device_clone)?;

        // Load components
        let whisper_encoder = WhisperEncoder::load(&audio_cfg, vb.pp("whisper_encoder"))?;

        let hidden_size = audio_cfg.d_model * config.downsample_factor();
        let audio_adapter = AudioLanguageAdapter::load(
            hidden_size,
            config.text_config().hidden_size,
            vb.pp("audio_language_adapter"),
        )?;

        let language_model =
            Qwen3Model::load(config.text_config().into(), vb.pp("language_model"))?;

        let time_embedding =
            TimeEmbedding::new(config.text_config().hidden_size, 10000.0, &device.device)?;

        info!("Loaded Voxtral Realtime model on {:?}", device.kind);

        Ok(Self {
            device,
            dtype,
            tokenizer,
            config,
            whisper_encoder,
            audio_adapter,
            language_model,
            time_embedding,
            mel,
        })
    }

    /// Transcribe audio (non-streaming)
    pub fn transcribe(
        &self,
        audio: &[f32],
        sample_rate: u32,
        _language: Option<&str>,
    ) -> Result<String> {
        // Resample to 16kHz if needed
        let audio = if sample_rate != 16_000 {
            resample_audio(audio, sample_rate, 16_000)?
        } else {
            audio.to_vec()
        };

        // Compute mel spectrogram
        let mel_spec = self.mel.compute(&audio)?;
        let n_mels = self.mel.config().n_mels;
        let frames = mel_spec.len();

        // Convert to tensor: [n_mels, frames] -> [1, n_mels, frames]
        let mut flat = Vec::with_capacity(frames * n_mels);
        for frame in mel_spec.iter() {
            flat.extend_from_slice(frame);
        }

        let mel = Tensor::from_vec(flat, (n_mels, frames), &self.device.device)?
            .transpose(0, 1)?
            .unsqueeze(0)? // [1, frames, n_mels]
            .to_dtype(self.dtype)?;

        // Process through whisper encoder
        let audio_embeds = self.whisper_encoder.forward(&mel)?;

        // Apply pooling
        let audio_embeds = self.pool_audio_embeddings(&audio_embeds)?;

        // Project to language model dimension
        let audio_embeds = self.audio_adapter.forward(&audio_embeds)?;

        // Build prompt
        let audio_tokens = audio_embeds.dim(1)?;
        let prompt_ids = self.tokenizer.build_transcription_prompt(audio_tokens);
        let input_ids = Tensor::from_vec(
            prompt_ids.clone(),
            (1, prompt_ids.len()),
            &self.device.device,
        )?;

        // Get text embeddings
        let text_embeds = self.language_model.embeddings(&input_ids)?;

        // Sum audio and text embeddings
        let combined_embeds = audio_embeds.broadcast_add(&text_embeds)?;

        // Apply time conditioning
        let time_tensor = Tensor::from_vec(
            vec![self.config.num_delay_tokens() as f32],
            (1,),
            &self.device.device,
        )?
        .to_dtype(self.dtype)?;
        let t_cond = self.time_embedding.forward(&time_tensor)?;

        // Generate
        let mut cache = Qwen3Cache::new(self.language_model.num_layers());
        let mut generated = Vec::new();
        let max_tokens = 1024usize;

        // Forward with audio - use forward_with_embeds for custom embeddings
        let combined_embeds = combined_embeds.broadcast_add(&t_cond.unsqueeze(0)?)?;
        let mut logits =
            self.language_model
                .forward_with_embeds(&combined_embeds, 0, Some(&mut cache), None)?;

        let specials = self.tokenizer.specials();
        let mut pos = combined_embeds.dim(1)?;

        for _ in 0..max_tokens {
            let next_logits = logits.i((0, logits.dim(1)? - 1))?;
            let next = argmax(&next_logits)?;

            if next == specials.eos || next == specials.end_audio {
                break;
            }

            generated.push(next);

            let next_tensor = Tensor::from_vec(vec![next], (1, 1), &self.device.device)?;
            logits = self
                .language_model
                .forward(&next_tensor, pos, Some(&mut cache))?;
            pos += 1;
        }

        // Decode
        self.tokenizer.decode_text(&generated)
    }

    /// Pool audio embeddings by block_size
    fn pool_audio_embeddings(&self, audio_embeds: &Tensor) -> Result<Tensor> {
        let (bsz, seq_len, hidden) = audio_embeds.dims3()?;
        let pool_size = self.config.block_pool_size();

        // Ensure seq_len is divisible by pool_size
        let new_len = seq_len / pool_size;
        let truncated_len = new_len * pool_size;

        if truncated_len < seq_len {
            let audio_embeds = audio_embeds.narrow(1, seq_len - truncated_len, truncated_len)?;
            let reshaped = audio_embeds.reshape((bsz, new_len, hidden * pool_size))?;
            Ok(reshaped)
        } else {
            let reshaped = audio_embeds.reshape((bsz, new_len, hidden * pool_size))?;
            Ok(reshaped)
        }
    }
}

/// Whisper encoder for audio processing
pub struct WhisperEncoder {
    conv1: candle_nn::Conv1d,
    conv2: candle_nn::Conv1d,
    layers: Vec<WhisperEncoderLayer>,
    ln_post: candle_nn::LayerNorm,
    embed_positions: Tensor,
    is_causal: bool,
}

impl WhisperEncoder {
    pub fn load(cfg: &super::config::AudioEncoderConfig, vb: VarBuilder) -> Result<Self> {
        let conv1_config = candle_nn::Conv1dConfig {
            stride: cfg.conv1_stride,
            padding: 1,
            groups: 1,
            dilation: 1,
            ..Default::default()
        };
        let conv2_config = candle_nn::Conv1dConfig {
            stride: cfg.conv2_stride,
            padding: 1,
            groups: 1,
            dilation: 1,
            ..Default::default()
        };

        let conv1 = candle_nn::conv1d(
            cfg.num_mel_bins,
            cfg.d_model,
            cfg.conv1_kernel_size,
            conv1_config,
            vb.pp("conv1"),
        )?;

        let conv2 = candle_nn::conv1d(
            cfg.d_model,
            cfg.d_model,
            cfg.conv2_kernel_size,
            conv2_config,
            vb.pp("conv2"),
        )?;

        let mut layers = Vec::with_capacity(cfg.encoder_layers);
        for i in 0..cfg.encoder_layers {
            layers.push(WhisperEncoderLayer::load(
                cfg,
                vb.pp(format!("layers.{i}")),
            )?);
        }

        let ln_post = candle_nn::layer_norm(cfg.d_model, 1e-5, vb.pp("ln_post"))?;

        // Sinusoidal position embeddings
        let max_len = cfg.max_source_positions;
        let hidden = cfg.d_model;
        let half_hidden = hidden / 2;
        let log_theta = (10000f32).ln() / (half_hidden as f32 - 1.0);
        let inv_freq: Vec<f32> = (0..half_hidden)
            .map(|i| (-log_theta * i as f32).exp())
            .collect();

        let mut pos_embed_data = Vec::with_capacity(max_len * hidden);
        for pos in 0..max_len {
            for i in 0..half_hidden {
                let timescale = inv_freq[i];
                pos_embed_data.push((pos as f32 * timescale).sin());
            }
            for i in 0..half_hidden {
                let timescale = inv_freq[i];
                pos_embed_data.push((pos as f32 * timescale).cos());
            }
        }

        let embed_positions = Tensor::from_vec(pos_embed_data, (max_len, hidden), vb.device())?;

        Ok(Self {
            conv1,
            conv2,
            layers,
            ln_post,
            embed_positions,
            is_causal: cfg.is_causal,
        })
    }

    pub fn forward(&self, input_features: &Tensor) -> Result<Tensor> {
        // input_features: [batch, frames, n_mels]
        // Transpose to [batch, n_mels, frames] for conv1d
        let x = input_features.transpose(1, 2)?;

        // Conv layers with gelu
        let x = self.conv1.forward(&x)?;
        let x = gelu(&x)?;
        let x = self.conv2.forward(&x)?;
        let x = gelu(&x)?;

        // Transpose back: [batch, hidden, frames] -> [batch, frames, hidden]
        let x = x.transpose(1, 2)?;

        // Add positional embeddings
        let seq_len = x.dim(1)?;
        let pos_embed = self.embed_positions.narrow(0, 0, seq_len)?;
        let pos_embed = pos_embed.unsqueeze(0)?.broadcast_as(x.shape())?;
        let x = x.broadcast_add(&pos_embed)?;

        // Transformer layers
        let mut x = x;
        for layer in &self.layers {
            x = layer.forward(&x, self.is_causal)?;
        }

        self.ln_post
            .forward(&x)
            .map_err(|e| Error::InferenceError(e.to_string()))
    }

    /// Forward only conv layers (for realtime processing)
    pub fn forward_conv(&self, mel_features: &[Tensor]) -> Result<Tensor> {
        let mut outputs = Vec::with_capacity(mel_features.len());

        for mel in mel_features {
            // mel: [n_mels, seq_len] -> [1, n_mels, seq_len]
            let x = mel.unsqueeze(0)?.transpose(1, 2)?;
            let x = self.conv1.forward(&x)?;
            let x = gelu(&x)?;
            let x = self.conv2.forward(&x)?;
            let x = gelu(&x)?;
            // [1, hidden, seq_len] -> [hidden, seq_len]
            let x = x.squeeze(0)?;
            outputs.push(x);
        }

        // Concatenate along sequence dimension
        let outputs_refs: Vec<&Tensor> = outputs.iter().collect();
        Tensor::cat(&outputs_refs, 1).map_err(|e| Error::InferenceError(e.to_string()))
    }
}

/// Whisper encoder layer
struct WhisperEncoderLayer {
    self_attn_layer_norm: candle_nn::LayerNorm,
    self_attn: WhisperAttention,
    final_layer_norm: candle_nn::LayerNorm,
    fc1: candle_nn::Linear,
    fc2: candle_nn::Linear,
}

impl WhisperEncoderLayer {
    pub fn load(cfg: &super::config::AudioEncoderConfig, vb: VarBuilder) -> Result<Self> {
        let self_attn_layer_norm =
            candle_nn::layer_norm(cfg.d_model, 1e-5, vb.pp("self_attn_layer_norm"))?;
        let self_attn = WhisperAttention::load(cfg, vb.pp("self_attn"))?;
        let final_layer_norm = candle_nn::layer_norm(cfg.d_model, 1e-5, vb.pp("final_layer_norm"))?;
        let fc1 = candle_nn::linear(cfg.d_model, cfg.encoder_ffn_dim, vb.pp("fc1"))?;
        let fc2 = candle_nn::linear(cfg.encoder_ffn_dim, cfg.d_model, vb.pp("fc2"))?;

        Ok(Self {
            self_attn_layer_norm,
            self_attn,
            final_layer_norm,
            fc1,
            fc2,
        })
    }

    pub fn forward(&self, x: &Tensor, is_causal: bool) -> Result<Tensor> {
        let residual = x;
        let x = self.self_attn_layer_norm.forward(x)?;
        let x = self.self_attn.forward(&x, is_causal)?;
        let x = (residual + x)?;

        let residual = &x;
        let x = self.final_layer_norm.forward(&x)?;
        let x = self.fc1.forward(&x)?;
        let x = gelu(&x)?;
        let x = self.fc2.forward(&x)?;
        residual
            .broadcast_add(&x)
            .map_err(|e| Error::InferenceError(e.to_string()))
    }
}

/// Whisper attention with optional causal masking
struct WhisperAttention {
    q_proj: candle_nn::Linear,
    k_proj: candle_nn::Linear,
    v_proj: candle_nn::Linear,
    out_proj: candle_nn::Linear,
    num_heads: usize,
    head_dim: usize,
    scale: f64,
}

impl WhisperAttention {
    pub fn load(cfg: &super::config::AudioEncoderConfig, vb: VarBuilder) -> Result<Self> {
        let head_dim = cfg.d_model / cfg.encoder_attention_heads;

        let q_proj = candle_nn::linear(cfg.d_model, cfg.d_model, vb.pp("q_proj"))?;
        let k_proj = candle_nn::linear(cfg.d_model, cfg.d_model, vb.pp("k_proj"))?;
        let v_proj = candle_nn::linear(cfg.d_model, cfg.d_model, vb.pp("v_proj"))?;
        let out_proj = candle_nn::linear(cfg.d_model, cfg.d_model, vb.pp("out_proj"))?;

        let scale = (head_dim as f64).powf(-0.5);

        Ok(Self {
            q_proj,
            k_proj,
            v_proj,
            out_proj,
            num_heads: cfg.encoder_attention_heads,
            head_dim,
            scale,
        })
    }

    pub fn forward(&self, x: &Tensor, is_causal: bool) -> Result<Tensor> {
        let (bsz, seq_len, _) = x.dims3()?;

        let q = self.q_proj.forward(x)?;
        let k = self.k_proj.forward(x)?;
        let v = self.v_proj.forward(x)?;

        // Reshape for multi-head attention
        let q = q
            .reshape((bsz, seq_len, self.num_heads, self.head_dim))?
            .transpose(1, 2)?;
        let k = k
            .reshape((bsz, seq_len, self.num_heads, self.head_dim))?
            .transpose(1, 2)?;
        let v = v
            .reshape((bsz, seq_len, self.num_heads, self.head_dim))?
            .transpose(1, 2)?;

        // Scaled dot-product attention
        let q = q.reshape((bsz * self.num_heads, seq_len, self.head_dim))?;
        let k = k.reshape((bsz * self.num_heads, seq_len, self.head_dim))?;
        let v = v.reshape((bsz * self.num_heads, seq_len, self.head_dim))?;

        let attn_weights = q.matmul(&k.transpose(1, 2)?)?;
        let attn_weights = attn_weights.broadcast_mul(&Tensor::from_vec(
            vec![self.scale as f32],
            (1, 1, 1),
            attn_weights.device(),
        )?)?;

        // Apply causal mask if needed
        let attn_weights = if is_causal {
            let mask = create_causal_mask(seq_len, x.device(), attn_weights.dtype())?;
            attn_weights.broadcast_add(&mask)?
        } else {
            attn_weights
        };

        let attn_weights = candle_nn::ops::softmax(&attn_weights, 2)?;
        let attn_output = attn_weights.matmul(&v)?;

        // Reshape back
        let attn_output = attn_output
            .reshape((bsz, self.num_heads, seq_len, self.head_dim))?
            .transpose(1, 2)?
            .reshape((bsz, seq_len, self.num_heads * self.head_dim))?;

        self.out_proj
            .forward(&attn_output)
            .map_err(|e| Error::InferenceError(e.to_string()))
    }
}

fn create_causal_mask(
    seq_len: usize,
    device: &candle_core::Device,
    dtype: DType,
) -> Result<Tensor> {
    let mut mask = vec![0.0f32; seq_len * seq_len];
    for i in 0..seq_len {
        for j in (i + 1)..seq_len {
            mask[i * seq_len + j] = f32::MIN;
        }
    }
    let mask_tensor = Tensor::from_vec(mask, (seq_len, seq_len), device)?;
    mask_tensor
        .to_dtype(dtype)
        .map_err(|e| Error::InferenceError(e.to_string()))
}

fn argmax(logits: &Tensor) -> Result<u32> {
    let logits = logits.to_vec1::<f32>()?;
    let mut max_idx = 0;
    let mut max_val = logits[0];
    for (i, &v) in logits.iter().enumerate() {
        if v > max_val {
            max_val = v;
            max_idx = i;
        }
    }
    Ok(max_idx as u32)
}

fn resample_audio(audio: &[f32], from_rate: u32, to_rate: u32) -> Result<Vec<f32>> {
    if from_rate == to_rate {
        return Ok(audio.to_vec());
    }

    let ratio = to_rate as f64 / from_rate as f64;
    let new_len = (audio.len() as f64 * ratio) as usize;
    let mut resampled = Vec::with_capacity(new_len);

    for i in 0..new_len {
        let src_idx = i as f64 / ratio;
        let src_idx_floor = src_idx.floor() as usize;
        let src_idx_ceil = (src_idx_floor + 1).min(audio.len() - 1);
        let frac = src_idx - src_idx_floor as f64;

        let val = audio[src_idx_floor] as f64 * (1.0 - frac) + audio[src_idx_ceil] as f64 * frac;
        resampled.push(val as f32);
    }

    Ok(resampled)
}

fn load_weights<'a>(
    model_dir: &'a Path,
    dtype: DType,
    device: &'a DeviceProfile,
) -> Result<VarBuilder<'a>> {
    // Voxtral uses consolidated.safetensors (single file)
    let consolidated_path = model_dir.join("consolidated.safetensors");
    if consolidated_path.exists() {
        info!("Loading Voxtral from consolidated.safetensors");
        unsafe {
            return VarBuilder::from_mmaped_safetensors(
                &[consolidated_path],
                dtype,
                &device.device,
            )
            .map_err(|e| Error::ModelLoadError(format!("Failed to load weights: {}", e)));
        }
    }

    let index_path = model_dir.join("model.safetensors.index.json");

    if index_path.exists() {
        let index_data = std::fs::read_to_string(&index_path)
            .map_err(|e| Error::ModelLoadError(format!("Failed to read index: {}", e)))?;
        let index: serde_json::Value = serde_json::from_str(&index_data)
            .map_err(|e| Error::ModelLoadError(format!("Failed to parse index: {}", e)))?;

        let weight_map = index
            .get("weight_map")
            .and_then(|m| m.as_object())
            .ok_or_else(|| Error::InvalidInput("Invalid weight map".to_string()))?;

        let mut shard_files: Vec<String> = weight_map
            .values()
            .filter_map(|v| v.as_str().map(String::from))
            .collect();
        shard_files.sort();
        shard_files.dedup();

        let shard_paths: Vec<std::path::PathBuf> =
            shard_files.iter().map(|f| model_dir.join(f)).collect();

        info!("Loading Voxtral with {} shard files", shard_paths.len());

        unsafe {
            VarBuilder::from_mmaped_safetensors(&shard_paths, dtype, &device.device)
                .map_err(|e| Error::ModelLoadError(format!("Failed to load shards: {}", e)))
        }
    } else {
        let weights_path = model_dir.join("model.safetensors");
        unsafe {
            VarBuilder::from_mmaped_safetensors(&[weights_path], dtype, &device.device)
                .map_err(|e| Error::ModelLoadError(format!("Failed to load weights: {}", e)))
        }
    }
}

/// GELU activation function
fn gelu(x: &Tensor) -> Result<Tensor> {
    let coeff = 0.044715f64;
    let sqrt_2_over_pi = (2.0f64 / std::f64::consts::PI).sqrt();
    let dtype = x.dtype();
    let x_f64 = x.to_dtype(candle_core::DType::F64)?;
    let x3 = x_f64.powf(3.0)?;
    let coeff_t = Tensor::from_vec(vec![coeff], (1,), x.device())?;
    let x3 = x3.broadcast_mul(&coeff_t)?;
    let sqrt_t = Tensor::from_vec(vec![sqrt_2_over_pi], (1,), x.device())?;
    let inner = (&x_f64 + x3)?.broadcast_mul(&sqrt_t)?;
    let tanh = inner.tanh()?;
    let one = Tensor::from_vec(vec![1.0f64], (1,), x.device())?;
    let half = Tensor::from_vec(vec![0.5f64], (1,), x.device())?;
    let out = x_f64.broadcast_mul(&one.broadcast_add(&tanh)?)?;
    let out = out.broadcast_mul(&half)?;
    out.to_dtype(dtype)
        .map_err(|e| Error::InferenceError(e.to_string()))
}
