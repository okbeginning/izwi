//! OpenAI-compatible speech synthesis endpoints.

use axum::{
    body::Body,
    extract::State,
    http::{header, StatusCode},
    response::Response,
    Json,
};
use base64::Engine;
use serde::{Deserialize, Serialize};
use std::convert::Infallible;
use std::time::{Duration, Instant};
use tokio::sync::mpsc;
use tracing::info;

use crate::error::ApiError;
use crate::state::AppState;
use izwi_core::audio::AudioFormat;
use izwi_core::{parse_tts_model_variant, AudioChunk, GenerationConfig, GenerationRequest};

/// OpenAI-compatible speech synthesis request.
#[derive(Debug, Deserialize)]
pub struct SpeechRequest {
    /// OpenAI field name for TTS model.
    pub model: String,
    /// OpenAI field name for text input.
    pub input: String,
    /// OpenAI-style voice selection.
    #[serde(default)]
    pub voice: Option<String>,
    /// OpenAI response format (`wav` and `pcm` currently supported by local runtime).
    #[serde(default)]
    pub response_format: Option<String>,
    /// OpenAI speed.
    #[serde(default)]
    pub speed: Option<f32>,
    /// Optional language hint (e.g. "Auto", "English", "Chinese").
    #[serde(default)]
    pub language: Option<String>,
    /// Optional sampling temperature.
    #[serde(default)]
    pub temperature: Option<f32>,
    /// Optional max token budget.
    #[serde(default)]
    pub max_tokens: Option<usize>,
    /// Alias for max output tokens in newer APIs.
    #[serde(default)]
    pub max_output_tokens: Option<usize>,
    /// Optional top-k sampling for model-specific runtimes.
    #[serde(default)]
    pub top_k: Option<usize>,
    /// If true, stream chunked audio from same endpoint.
    #[serde(default)]
    pub stream: Option<bool>,
    /// Optional voice design prompt.
    #[serde(default)]
    pub instructions: Option<String>,
    /// Optional reference audio (base64) for voice cloning.
    #[serde(default)]
    pub reference_audio: Option<String>,
    /// Optional reference transcript for cloning.
    #[serde(default)]
    pub reference_text: Option<String>,
}

#[derive(Debug, Serialize)]
struct SpeechStreamEvent {
    event: &'static str,
    #[serde(skip_serializing_if = "Option::is_none")]
    request_id: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    sequence: Option<usize>,
    #[serde(skip_serializing_if = "Option::is_none")]
    audio_base64: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    sample_count: Option<usize>,
    #[serde(skip_serializing_if = "Option::is_none")]
    is_final: Option<bool>,
    #[serde(skip_serializing_if = "Option::is_none")]
    sample_rate: Option<u32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    audio_format: Option<&'static str>,
    #[serde(skip_serializing_if = "Option::is_none")]
    tokens_generated: Option<usize>,
    #[serde(skip_serializing_if = "Option::is_none")]
    generation_time_ms: Option<f32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    audio_duration_secs: Option<f32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    rtf: Option<f32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    error: Option<String>,
}

pub async fn speech(
    State(state): State<AppState>,
    Json(req): Json<SpeechRequest>,
) -> Result<Response<Body>, ApiError> {
    info!("OpenAI speech request: {} chars", req.input.len());

    let variant = parse_tts_model_variant(&req.model)
        .map_err(|err| ApiError::bad_request(format!("Unsupported TTS model: {}", err)))?;
    state.engine.load_model(variant).await?;

    if req.stream.unwrap_or(false) {
        return stream_speech(state, req).await;
    }

    let _permit = state.acquire_permit().await;

    let timeout = Duration::from_secs(state.request_timeout_secs);
    let format = parse_response_format(req.response_format.as_deref().unwrap_or("wav"))?;

    let result = tokio::time::timeout(timeout, async {
        let mut gen_config = GenerationConfig {
            streaming: false,
            ..GenerationConfig::default()
        };
        if let Some(temp) = req.temperature {
            gen_config.temperature = temp;
        }
        if let Some(speed) = req.speed {
            gen_config.speed = speed;
        }
        if let Some(max_tokens) = req.max_output_tokens.or(req.max_tokens) {
            gen_config.max_tokens = max_tokens;
        }
        if let Some(top_k) = req.top_k {
            gen_config.top_k = top_k;
        }
        gen_config.speaker = req.voice.clone();

        let gen_request = GenerationRequest {
            id: uuid::Uuid::new_v4().to_string(),
            text: req.input.clone(),
            config: gen_config,
            language: req.language.clone(),
            reference_audio: req.reference_audio.clone(),
            reference_text: req.reference_text.clone(),
            voice_description: req.instructions.clone(),
        };

        state.engine.generate(gen_request).await
    })
    .await
    .map_err(|_| ApiError::internal("Request timeout"))??;

    let encoder = state.engine.audio_encoder().await;
    let samples = result.samples.clone();
    let audio_bytes = tokio::task::spawn_blocking(move || encoder.encode(&samples, format))
        .await
        .map_err(|e| ApiError::internal(format!("Audio encoding failed: {}", e)))??;

    let content_type = izwi_core::audio::AudioEncoder::content_type(format);
    let duration_secs = result.duration_secs();
    let generation_time_ms = result.total_time_ms;
    let rtf = result.rtf();
    let tokens_generated = result.total_tokens;

    Ok(Response::builder()
        .header(header::CONTENT_TYPE, content_type)
        .header("X-Generation-Time-Ms", format!("{:.1}", generation_time_ms))
        .header("X-Audio-Duration-Secs", format!("{:.2}", duration_secs))
        .header("X-RTF", format!("{:.3}", rtf))
        .header("X-Tokens-Generated", tokens_generated.to_string())
        .header(
            "Access-Control-Expose-Headers",
            "X-Generation-Time-Ms, X-Audio-Duration-Secs, X-RTF, X-Tokens-Generated",
        )
        .body(Body::from(audio_bytes))
        .unwrap())
}

async fn stream_speech(state: AppState, req: SpeechRequest) -> Result<Response<Body>, ApiError> {
    let format = parse_response_format(req.response_format.as_deref().unwrap_or("pcm"))?;

    let mut gen_config = GenerationConfig {
        streaming: true,
        ..GenerationConfig::default()
    };
    if let Some(temp) = req.temperature {
        gen_config.temperature = temp;
    }
    if let Some(speed) = req.speed {
        gen_config.speed = speed;
    }
    if let Some(max_tokens) = req.max_output_tokens.or(req.max_tokens) {
        gen_config.max_tokens = max_tokens;
    }
    if let Some(top_k) = req.top_k {
        gen_config.top_k = top_k;
    }
    gen_config.speaker = req.voice.clone();

    let gen_request = GenerationRequest {
        id: uuid::Uuid::new_v4().to_string(),
        text: req.input.clone(),
        config: gen_config,
        language: req.language.clone(),
        reference_audio: req.reference_audio.clone(),
        reference_text: req.reference_text.clone(),
        voice_description: req.instructions.clone(),
    };

    let stream_request_id = gen_request.id.clone();
    let stream_audio_format = stream_audio_format_label(format);
    let (event_tx, mut event_rx) = mpsc::unbounded_channel::<String>();

    let engine = state.engine.clone();
    let semaphore = state.request_semaphore.clone();
    let timeout = Duration::from_secs(state.request_timeout_secs);
    tokio::spawn(async move {
        let _permit = match semaphore.acquire_owned().await {
            Ok(permit) => permit,
            Err(_) => {
                let error_event = SpeechStreamEvent {
                    event: "error",
                    request_id: Some(stream_request_id.clone()),
                    sequence: None,
                    audio_base64: None,
                    sample_count: None,
                    is_final: None,
                    sample_rate: None,
                    audio_format: None,
                    tokens_generated: None,
                    generation_time_ms: None,
                    audio_duration_secs: None,
                    rtf: None,
                    error: Some("Server is shutting down".to_string()),
                };
                let _ = event_tx.send(serde_json::to_string(&error_event).unwrap_or_default());

                let done_event = SpeechStreamEvent {
                    event: "done",
                    request_id: Some(stream_request_id),
                    sequence: None,
                    audio_base64: None,
                    sample_count: None,
                    is_final: None,
                    sample_rate: None,
                    audio_format: None,
                    tokens_generated: None,
                    generation_time_ms: None,
                    audio_duration_secs: None,
                    rtf: None,
                    error: None,
                };
                let _ = event_tx.send(serde_json::to_string(&done_event).unwrap_or_default());
                return;
            }
        };

        let sample_rate = engine.sample_rate().await;
        let start_event = SpeechStreamEvent {
            event: "start",
            request_id: Some(stream_request_id.clone()),
            sequence: None,
            audio_base64: None,
            sample_count: None,
            is_final: None,
            sample_rate: Some(sample_rate),
            audio_format: Some(stream_audio_format),
            tokens_generated: None,
            generation_time_ms: None,
            audio_duration_secs: None,
            rtf: None,
            error: None,
        };
        let _ = event_tx.send(serde_json::to_string(&start_event).unwrap_or_default());

        let (chunk_tx, mut chunk_rx) = mpsc::channel::<AudioChunk>(32);
        let generation_engine = engine.clone();
        let generation_task = tokio::spawn(async move {
            tokio::time::timeout(
                timeout,
                generation_engine.generate_streaming(gen_request, chunk_tx),
            )
            .await
        });

        let mut total_samples = 0usize;
        let stream_started = Instant::now();
        let encoder = izwi_core::audio::AudioEncoder::new(sample_rate, 1);

        while let Some(chunk) = chunk_rx.recv().await {
            if chunk.samples.is_empty() {
                continue;
            }

            total_samples += chunk.samples.len();
            let bytes = match encoder.encode(&chunk.samples, format) {
                Ok(bytes) => bytes,
                Err(err) => {
                    let error_event = SpeechStreamEvent {
                        event: "error",
                        request_id: Some(stream_request_id.clone()),
                        sequence: None,
                        audio_base64: None,
                        sample_count: None,
                        is_final: None,
                        sample_rate: None,
                        audio_format: None,
                        tokens_generated: None,
                        generation_time_ms: None,
                        audio_duration_secs: None,
                        rtf: None,
                        error: Some(format!("Failed to encode audio chunk: {}", err)),
                    };
                    let _ = event_tx.send(serde_json::to_string(&error_event).unwrap_or_default());
                    break;
                }
            };

            let chunk_event = SpeechStreamEvent {
                event: "chunk",
                request_id: Some(chunk.request_id.clone()),
                sequence: Some(chunk.sequence),
                audio_base64: Some(base64::engine::general_purpose::STANDARD.encode(bytes)),
                sample_count: Some(chunk.samples.len()),
                is_final: Some(chunk.is_final),
                sample_rate: None,
                audio_format: None,
                tokens_generated: None,
                generation_time_ms: None,
                audio_duration_secs: None,
                rtf: None,
                error: None,
            };
            let _ = event_tx.send(serde_json::to_string(&chunk_event).unwrap_or_default());
        }

        let generation_outcome = generation_task.await;
        match generation_outcome {
            Ok(Ok(Ok(()))) => {
                let generation_time_ms = stream_started.elapsed().as_secs_f32() * 1000.0;
                let audio_duration_secs = total_samples as f32 / sample_rate as f32;
                let tokens_generated = total_samples / 256;
                let rtf = if audio_duration_secs > 0.0 {
                    (generation_time_ms / 1000.0) / audio_duration_secs
                } else {
                    0.0
                };

                let final_event = SpeechStreamEvent {
                    event: "final",
                    request_id: Some(stream_request_id.clone()),
                    sequence: None,
                    audio_base64: None,
                    sample_count: None,
                    is_final: None,
                    sample_rate: None,
                    audio_format: None,
                    tokens_generated: Some(tokens_generated),
                    generation_time_ms: Some(generation_time_ms),
                    audio_duration_secs: Some(audio_duration_secs),
                    rtf: Some(rtf),
                    error: None,
                };
                let _ = event_tx.send(serde_json::to_string(&final_event).unwrap_or_default());
            }
            Ok(Ok(Err(err))) => {
                let error_event = SpeechStreamEvent {
                    event: "error",
                    request_id: Some(stream_request_id.clone()),
                    sequence: None,
                    audio_base64: None,
                    sample_count: None,
                    is_final: None,
                    sample_rate: None,
                    audio_format: None,
                    tokens_generated: None,
                    generation_time_ms: None,
                    audio_duration_secs: None,
                    rtf: None,
                    error: Some(err.to_string()),
                };
                let _ = event_tx.send(serde_json::to_string(&error_event).unwrap_or_default());
            }
            Ok(Err(_)) => {
                let error_event = SpeechStreamEvent {
                    event: "error",
                    request_id: Some(stream_request_id.clone()),
                    sequence: None,
                    audio_base64: None,
                    sample_count: None,
                    is_final: None,
                    sample_rate: None,
                    audio_format: None,
                    tokens_generated: None,
                    generation_time_ms: None,
                    audio_duration_secs: None,
                    rtf: None,
                    error: Some("Speech synthesis request timed out".to_string()),
                };
                let _ = event_tx.send(serde_json::to_string(&error_event).unwrap_or_default());
            }
            Err(err) => {
                let error_event = SpeechStreamEvent {
                    event: "error",
                    request_id: Some(stream_request_id.clone()),
                    sequence: None,
                    audio_base64: None,
                    sample_count: None,
                    is_final: None,
                    sample_rate: None,
                    audio_format: None,
                    tokens_generated: None,
                    generation_time_ms: None,
                    audio_duration_secs: None,
                    rtf: None,
                    error: Some(format!("Streaming task failed: {}", err)),
                };
                let _ = event_tx.send(serde_json::to_string(&error_event).unwrap_or_default());
            }
        }

        let done_event = SpeechStreamEvent {
            event: "done",
            request_id: Some(stream_request_id),
            sequence: None,
            audio_base64: None,
            sample_count: None,
            is_final: None,
            sample_rate: None,
            audio_format: None,
            tokens_generated: None,
            generation_time_ms: None,
            audio_duration_secs: None,
            rtf: None,
            error: None,
        };
        let _ = event_tx.send(serde_json::to_string(&done_event).unwrap_or_default());
    });

    let stream = async_stream::stream! {
        while let Some(payload) = event_rx.recv().await {
            yield Ok::<_, Infallible>(format!("data: {payload}\n\n"));
        }
    };

    Ok(Response::builder()
        .status(StatusCode::OK)
        .header(header::CONTENT_TYPE, "text/event-stream")
        .header(header::CACHE_CONTROL, "no-cache")
        .body(Body::from_stream(stream))
        .unwrap())
}

fn parse_response_format(format: &str) -> Result<AudioFormat, ApiError> {
    match format.to_ascii_lowercase().as_str() {
        "wav" => Ok(AudioFormat::Wav),
        "pcm" | "pcm16" | "pcm_i16" | "raw_i16" => Ok(AudioFormat::RawI16),
        "raw_f32" | "pcm_f32" => Ok(AudioFormat::RawF32),
        // Accepted OpenAI names that are not yet supported by local encoder.
        "mp3" | "opus" | "aac" | "flac" => Err(ApiError::bad_request(format!(
            "Unsupported response_format: {}. This runtime currently supports wav and pcm",
            format
        ))),
        unsupported => Err(ApiError::bad_request(format!(
            "Unsupported response_format: {}. Supported formats: wav, pcm",
            unsupported
        ))),
    }
}

fn stream_audio_format_label(format: AudioFormat) -> &'static str {
    match format {
        AudioFormat::Wav => "wav",
        AudioFormat::RawF32 => "pcm_f32",
        AudioFormat::RawI16 => "pcm_i16",
    }
}
